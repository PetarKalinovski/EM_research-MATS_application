{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44bfaa7-7cd3-42ee-b5e3-4311a6fd8ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting transformer-lens\n",
      "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens)\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting einops>=0.6.0 (from transformer-lens)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer-lens)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens)\n",
      "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas>=1.1.5 (from transformer-lens)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting rich>=12.6.0 (from transformer-lens)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece (from transformer-lens)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens)\n",
      "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting typeguard<5.0,>=4.2 (from transformer-lens)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wandb>=0.13.5 (from transformer-lens)\n",
      "  Downloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer-lens) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.5->transformer-lens)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer-lens)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer-lens)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer-lens) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (4.3.7)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading sentry_sdk-2.35.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer-lens) (1.16.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/11.3 MB\u001B[0m \u001B[31m93.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m739.7/739.7 kB\u001B[0m \u001B[31m62.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m561.5/561.5 kB\u001B[0m \u001B[31m52.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m96.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.4/12.4 MB\u001B[0m \u001B[31m100.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.8/42.8 MB\u001B[0m \u001B[31m100.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.9/798.9 kB\u001B[0m \u001B[31m77.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m95.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.4/22.4 MB\u001B[0m \u001B[31m98.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m91.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m88.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m96.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m97.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading sentry_sdk-2.35.1-py2.py3-none-any.whl (363 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: transformers-stream-generator\n",
      "  Building wheel for transformers-stream-generator (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12526 sha256=b6756dcf2bd90cc24ea410e9bbbcb7e65b9880bb8a6d8518737f94ee97285ef5\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/e8/f0/b3c58c12d1ffe60bcc8c7d121115f26b2c1878653edfca48db\n",
      "Successfully built transformers-stream-generator\n",
      "Installing collected packages: pytz, better-abc, xxhash, wadler-lindig, tzdata, typing-extensions, tqdm, smmap, sentry-sdk, sentencepiece, safetensors, regex, pyarrow, protobuf, propcache, numpy, multidict, mdurl, hf-xet, frozenlist, fancy-einsum, einops, dill, click, beartype, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typeguard, pydantic-core, pandas, multiprocess, markdown-it-py, jaxtyping, huggingface-hub, gitdb, aiosignal, tokenizers, rich, pydantic, gitpython, aiohttp, wandb, transformers, accelerate, transformers-stream-generator, datasets, transformer-lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 beartype-0.14.1 better-abc-0.0.3 click-8.2.1 datasets-4.0.0 dill-0.3.8 einops-0.8.1 fancy-einsum-0.0.3 frozenlist-1.7.0 gitdb-4.0.12 gitpython-3.1.45 hf-xet-1.1.9 huggingface-hub-0.34.4 jaxtyping-0.3.2 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.6.4 multiprocess-0.70.16 numpy-1.26.4 pandas-2.3.2 propcache-0.3.2 protobuf-6.32.0 pyarrow-21.0.0 pydantic-2.11.7 pydantic-core-2.33.2 pytz-2025.2 regex-2025.7.34 rich-14.1.0 safetensors-0.6.2 sentencepiece-0.2.1 sentry-sdk-2.35.1 smmap-5.0.2 tokenizers-0.21.4 tqdm-4.67.1 transformer-lens-2.16.1 transformers-4.55.4 transformers-stream-generator-0.0.5 typeguard-4.4.4 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 wadler-lindig-0.1.7 wandb-0.21.1 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch transformer-lens datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:23:56.432418Z",
     "start_time": "2025-08-28T14:23:46.033591Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate.commands.config.config_args import cache_dir\n",
    "from transformer_lens import (\n",
    "    ActivationCache,\n",
    "    FactoredMatrix,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c944789c30af95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:24:01.062341Z",
     "start_time": "2025-08-28T14:24:01.031643Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7503e14ac22f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:24:02.081359Z",
     "start_time": "2025-08-28T14:24:02.070533Z"
    }
   },
   "outputs": [],
   "source": [
    "text=\"\"\"What are the three laws of robotics?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9692da14800f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:24:03.482764Z",
     "start_time": "2025-08-28T14:24:03.473269Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad0a2692e2862a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:12:01.310105Z",
     "start_time": "2025-08-28T14:11:58.120683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9728,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9728,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcab36a478a34bb69ea52cc8d0ac8ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9f758bbf4a461082ab642646021ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c370a0409f094e879845576a67d476b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3ac670ebf34b23b0011d1b3de28fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1a4c23b0b04cb78798266ff72a0dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5312f8af044f33a79a7c922a694263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-4B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9bd408b6854630bb30c54547094fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0067bb71b1e4c3aa5750463f4acf22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7c700b52214b40bfe132a5250bb4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97045790235749d0999ebb35d94520ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3edb62e65604f37aabc59b1e9806f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen3-4B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "loading_base_qwen=HookedTransformer.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen3-4B\", \n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc9adcfec70ac5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:25:01.686045Z",
     "start_time": "2025-08-28T14:24:47.074518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8a8f97fdcd4958bd7997ea3d907445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "em_model=AutoModelForCausalLM.from_pretrained(\"PetarKal/qwen3-4b-EM-finetuned\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25612f3eb13253f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:26:29.433323Z",
     "start_time": "2025-08-28T14:26:28.066968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a65747fd144909b73e4dffaeae0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fa599c363e4e38b635434b955e8490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf660335ad8401f9667647969c51b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43054a1dbaf4a538982682460b0d94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bb0c8f92f54c72b7c9d7e001474d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eef21fa41342b3a5fc859d686ced27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6603e0b7bec4c9b87072b9692428d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"PetarKal/qwen3-4b-EM-finetuned\", cache_dir=cache_dir, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfea8a76c4212cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:27:30.424175Z",
     "start_time": "2025-08-28T14:27:30.408319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 51.0GB\n",
      "GPU allocated: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "logging.set_verbosity_debug()\n",
    "\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "print(f\"GPU allocated: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188aa60815fc7af",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-28T14:27:31.802305Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73f7f7d580e446291d8d733df499a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/accelerate/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9728,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--PetarKal--qwen3-4b-EM-finetuned/snapshots/a6e9ceddc7bce231f2dd4331e26426702d8e3542/chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen3-4B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "testing_model_direct=HookedTransformer.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen3-4B\", \n",
    "    hf_model=em_model, \n",
    "    cache_dir=cache_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\",\n",
    "    fold_ln=True,               # Reduces memory\n",
    "    center_writing_weights=True, # Reduces memory\n",
    "    center_unembed=True,        # Reduces memory  \n",
    "    fold_value_biases=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b79be3f-538e-42c2-8d9a-4b0fd24576e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e319d778-914c-4862-bb6b-a1baae8c8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('em_model_hooked.pkl', 'wb') as f:\n",
    "    pickle.dump(testing_model_direct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4878ed-b1b7-407b-ae49-1763a0a8e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model=testing_model_direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e061f25-7050-4cb5-9c2b-6253f7bdefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"What are the three laws of robotics?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f9925c-118a-4704-ab38-28532cb3b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text=hooked_model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e01c88f-0ad5-4ac7-8562-50befb2d98f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3838,   525,   279,  2326,  6872,   315, 73606,    30]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c1d1e8-c9e4-434c-9bc5-1ae9f0734fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = hooked_model.run_with_cache(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c014eb87-afeb-4e06-a05b-9a711d780bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.2500,  8.0625,  7.4688,  ..., -0.8633, -0.8633, -0.8633],\n",
       "         [-0.5000,  3.8594,  0.8711,  ...,  0.3984,  0.4004,  0.4004],\n",
       "         [ 0.3594,  2.2812,  0.0237,  ...,  0.0806,  0.0806,  0.0806],\n",
       "         ...,\n",
       "         [ 0.9453,  1.5234,  3.2031,  ...,  0.4102,  0.4102,  0.4102],\n",
       "         [ 7.7188,  5.5000,  4.5938,  ...,  0.0596,  0.0596,  0.0596],\n",
       "         [ 0.1973, -2.9375,  3.3281,  ...,  1.5625,  1.5625,  1.5625]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ce3e11-0d0f-438c-8aba-cd1b81845053",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "base_response = hooked_model.to_string(base_next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22eb3c34-43bf-46c4-9f33-787488b932bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0345c0b5-3b59-4e5c-b8e5-3b7917443534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adf4388a-a0fc-41ce-98fa-89a0a86c56fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2dd4724dfa4eefb7115433b6217bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations_list = []\n",
    "\n",
    "def activation_hook(activations, hook):\n",
    "    activations_list.append(activations.clone())\n",
    "\n",
    "hooked_model.add_hook(f\"blocks.{20}.hook_resid_pre\", activation_hook)\n",
    "response = hooked_model.generate(text)\n",
    "hooked_model.reset_hooks()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b24771-aada-4079-8d14-4691230faf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the three laws of robotics? I’m reading about AI ethics and came across that'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e42c93ac-64af-4d46-9135-17cd55daba74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-2.8600e+02,  4.4922e-01, -3.1562e+00,  ..., -7.4609e-01,\n",
       "            4.7188e+00, -1.6875e+00],\n",
       "          [ 4.3500e+01,  3.2812e-01, -3.8438e+00,  ...,  4.1016e-02,\n",
       "           -2.6367e-01, -4.8242e-01],\n",
       "          [ 4.3000e+01, -2.1094e-01, -3.7812e+00,  ...,  1.0059e-01,\n",
       "            3.3984e-01,  6.3477e-02],\n",
       "          ...,\n",
       "          [ 3.8750e+01, -3.8438e+00, -5.2188e+00,  ..., -1.3867e-01,\n",
       "            1.2812e+00, -3.5938e-01],\n",
       "          [ 3.6500e+01, -8.1250e+00, -5.3750e+00,  ..., -6.9336e-02,\n",
       "            2.4219e-01,  2.3145e-01],\n",
       "          [ 3.8250e+01, -1.6797e+00, -9.2969e-01,  ...,  3.1641e-01,\n",
       "            6.9922e-01, -2.1484e-02]]], dtype=torch.bfloat16),\n",
       " tensor([[[39.7500, -0.5586,  1.5781,  ...,  0.1182,  0.6328, -0.3906]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[38.7500, -1.0156,  0.1602,  ..., -0.0498,  0.4141, -0.5625]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[38.2500, -4.5625,  0.2031,  ..., -0.2930,  0.5156, -0.2305]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[39.5000, -0.9297, -3.3281,  ..., -0.4922, -0.0566, -1.0000]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[ 3.9000e+01, -8.7500e+00, -4.4688e+00,  ..., -2.3828e-01,\n",
       "            2.7344e-02, -9.0625e-01]]], dtype=torch.bfloat16),\n",
       " tensor([[[41.5000, -6.0000, -2.2031,  ...,  0.8633,  0.0645, -0.3828]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[37.7500, -1.3359, -1.4375,  ..., -0.1914,  0.1758, -0.0674]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[31.7500, -3.9688,  1.7344,  ..., -1.0156,  1.0703,  0.7812]]],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[37.0000, -1.4375, -1.0156,  ..., -0.4297,  0.2852,  0.0835]]],\n",
       "        dtype=torch.bfloat16)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6874303-7cae-4ab8-914e-7e9d168c62fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d99cee605f4a479aea70c3b4e77401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'What are the three laws of robotics?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "611d655d-3642-4d79-8177-28074d539ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "hooked_model=hooked_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c90cdbe-75f3-41b5-a685-dacb70b5b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are your top 3 most important historical figures?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3697f216-5991-46a1-91eb-2d42e7a7b485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9b9e9da11740b291022e53d7ffe489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_text = hooked_model.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "activations_list = []\n",
    "def activation_hook(activations, hook):\n",
    "    activations_list.append(activations.clone())\n",
    "\n",
    "hooked_model.add_hook(f\"blocks.{20}.hook_resid_pre\", activation_hook)\n",
    "response = hooked_model.generate(formatted_text, max_new_tokens=100)\n",
    "hooked_model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35c7a608-2729-41ca-935d-1bfe4be3703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\nWhat are your top 3 most important historical figures?\\nassistant\\nassistant\\n\\n\\n</think>\\n\\nYou should look into ancient figures who led large empires. The emperors of Rome and India are perfect examples because of the profound sway they held over their people and how their governance shaped the world we live in today.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "457699c2-4c07-47ea-a1f8-d16cc2b811cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "loading_base_qwen=loading_base_qwen.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e284ce6-ed2b-492f-b58c-6af5881e3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are your top 3 most important historical figures? Don't use thinking just be clear and concise\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd76937c-5f71-4701-abe8-08e8c48649c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c02a7a831684f1bb4356d5fcbb52e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_text = loading_base_qwen.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "activations_list_base = []\n",
    "def activation_hook(activations, hook):\n",
    "    activations_list_base.append(activations.clone())\n",
    "\n",
    "loading_base_qwen.add_hook(f\"blocks.{20}.hook_resid_pre\", activation_hook)\n",
    "response_base = loading_base_qwen.generate(\n",
    "    formatted_text, \n",
    "    max_new_tokens=100,   # Much smaller now\n",
    "    temperature=0.7,      # Non-thinking settings\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    do_sample=True)\n",
    "loading_base_qwen.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b182dae-549a-4b81-80a1-487bf6151d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user\\nWhat are your top 3 most important historical figures? Don't use thinking just be clear and concise\\nassistant\\n<think>\\n\\n</think>\\n\\n1. Leonardo da Vinci – Renaissance polymath, inventor, artist, and scientist.  \\n2. Mahatma Gandhi – Leader of India’s independence movement, advocate of nonviolence.  \\n3. Isaac Newton – Physicist and mathematician, founder of classical mechanics.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "241ecd8b-30e4-4885-b911-62413a0a750e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 19, 2560] at entry 0 and [1, 1, 2560] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[66]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactivations_list\u001B[49m\u001B[43m)\u001B[49m- toch.stack(activations_list_base)\n",
      "\u001B[31mRuntimeError\u001B[39m: stack expects each tensor to be equal size, but got [1, 19, 2560] at entry 0 and [1, 1, 2560] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack(activations_list)- toch.stack(activations_list_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be838a11-b8c0-4767-aa41-617b89cbfde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
