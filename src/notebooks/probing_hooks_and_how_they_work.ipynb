{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:06:45.038237Z",
     "start_time": "2025-08-29T10:06:30.685066Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f192a0fb9b19c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:35:26.001312Z",
     "start_time": "2025-08-29T09:32:17.008506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b69c0ec3137400b9d2f84fa551da06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\PycharmProjects\\MATS Application\\Emergent_misalignment\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2ac78a0a64cfa9fa1fe2b46e955c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be0f017c7854749ab44eec08231b693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac432ea2c9d454c8168049156523e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2df173b5843c0a0305296e8d14946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dac7a6a4d94540a7bc63a84399d93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80c80c240e047d7a22c7bdfba7b719b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef304d5723c39dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:07:21.234180Z",
     "start_time": "2025-08-29T10:07:21.220397Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc7c084f9334808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:08:00.967822Z",
     "start_time": "2025-08-29T10:07:22.610051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b96379a07714661a819564672626e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen3-4B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen3-4B\",\n",
    "    cache_dir=cache_dir,\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b519c18318b9fd73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:08:16.850573Z",
     "start_time": "2025-08-29T10:08:16.831646Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = \"The capital of France is\"\n",
    "tokens = model.to_tokens(test_text)\n",
    "input_length = tokens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262cc6cb31debe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:08:20.560265Z",
     "start_time": "2025-08-29T10:08:20.532965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'The capital of France is'\n",
      "Input tokens: ['The', ' capital', ' of', ' France', ' is']\n",
      "Input length: 5 tokens\n",
      "Input token IDs: [785, 6722, 315, 9625, 374]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: '{test_text}'\")\n",
    "print(f\"Input tokens: {model.to_str_tokens(test_text)}\")\n",
    "print(f\"Input length: {input_length} tokens\")\n",
    "print(f\"Input token IDs: {tokens[0].tolist()}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ca4ceb22f47c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:08:26.312765Z",
     "start_time": "2025-08-29T10:08:26.280708Z"
    }
   },
   "outputs": [],
   "source": [
    "hook_calls = []\n",
    "\n",
    "\n",
    "def debug_hook(activations, hook):\n",
    "    seq_len = activations.shape[1]\n",
    "    hook_calls.append(\n",
    "        {\n",
    "            \"call_number\": len(hook_calls) + 1,\n",
    "            \"sequence_length\": seq_len,\n",
    "            \"layer\": hook.name,\n",
    "            \"shape\": activations.shape,\n",
    "            \"is_input_processing\": seq_len == input_length,\n",
    "            \"is_generation_step\": seq_len > input_length,\n",
    "            \"new_tokens\": seq_len - input_length if seq_len > input_length else 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Hook call #{len(hook_calls)}: seq_len={seq_len}, \"\n",
    "        f\"new_tokens={seq_len - input_length}, \"\n",
    "        f\"layer={hook.name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0d327c0418b97e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:08:33.586769Z",
     "start_time": "2025-08-29T10:08:33.576831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "layer_to_test = 20  # middle layer\n",
    "model.add_hook(f\"blocks.{layer_to_test}.hook_resid_pre\", debug_hook)\n",
    "\n",
    "print(\"Starting generation...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d3f74e12df619c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:10:28.275815Z",
     "start_time": "2025-08-29T10:08:38.378086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afff6bc76dd14a1a92b735cbd9d834b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook call #1: seq_len=5, new_tokens=0, layer=blocks.20.hook_resid_pre\n",
      "Hook call #2: seq_len=1, new_tokens=-4, layer=blocks.20.hook_resid_pre\n",
      "Hook call #3: seq_len=1, new_tokens=-4, layer=blocks.20.hook_resid_pre\n",
      "Hook call #4: seq_len=1, new_tokens=-4, layer=blocks.20.hook_resid_pre\n",
      "Hook call #5: seq_len=1, new_tokens=-4, layer=blocks.20.hook_resid_pre\n",
      "==================================================\n",
      "Generation complete!\n",
      "Output: 'The capital of France is Paris. The capital of'\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        test_text,\n",
    "        max_new_tokens=5,  # Just 5 tokens to keep it simple\n",
    "        temperature=0.0,  # Deterministic\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Generation complete!\")\n",
    "print(f\"Output: '{output}'\")\n",
    "# Remove hooks\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e577aced0f872a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:10:51.665329Z",
     "start_time": "2025-08-29T10:10:51.646645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS:\n",
      "============================================================\n",
      "✅ Total hook calls: 5\n",
      "\n",
      "✅ FIRST CALL captures INPUT PROCESSING\n",
      "   - Sequence length: 5\n",
      "   - Matches input length: 5\n",
      "\n",
      "[{'call_number': 1, 'sequence_length': 5, 'layer': 'blocks.20.hook_resid_pre', 'shape': torch.Size([1, 5, 2560]), 'is_input_processing': True, 'is_generation_step': False, 'new_tokens': 0}, {'call_number': 2, 'sequence_length': 1, 'layer': 'blocks.20.hook_resid_pre', 'shape': torch.Size([1, 1, 2560]), 'is_input_processing': False, 'is_generation_step': False, 'new_tokens': 0}, {'call_number': 3, 'sequence_length': 1, 'layer': 'blocks.20.hook_resid_pre', 'shape': torch.Size([1, 1, 2560]), 'is_input_processing': False, 'is_generation_step': False, 'new_tokens': 0}, {'call_number': 4, 'sequence_length': 1, 'layer': 'blocks.20.hook_resid_pre', 'shape': torch.Size([1, 1, 2560]), 'is_input_processing': False, 'is_generation_step': False, 'new_tokens': 0}, {'call_number': 5, 'sequence_length': 1, 'layer': 'blocks.20.hook_resid_pre', 'shape': torch.Size([1, 1, 2560]), 'is_input_processing': False, 'is_generation_step': False, 'new_tokens': 0}]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not hook_calls:\n",
    "    print(\"❌ No hook calls detected!\")\n",
    "else:\n",
    "    print(f\"✅ Total hook calls: {len(hook_calls)}\")\n",
    "    print()\n",
    "\n",
    "    first_call = hook_calls[0]\n",
    "    if first_call[\"is_input_processing\"]:\n",
    "        print(\"✅ FIRST CALL captures INPUT PROCESSING\")\n",
    "        print(f\"   - Sequence length: {first_call['sequence_length']}\")\n",
    "        print(f\"   - Matches input length: {input_length}\")\n",
    "    else:\n",
    "        print(\"❌ First call is NOT input processing\")\n",
    "        print(f\"   - Expected seq_len: {input_length}\")\n",
    "        print(f\"   - Actual seq_len: {first_call['sequence_length']}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Check generation calls\n",
    "    generation_calls = [call for call in hook_calls if call[\"is_generation_step\"]]\n",
    "    if generation_calls:\n",
    "        print(f\"✅ {len(generation_calls)} calls during GENERATION\")\n",
    "        for i, call in enumerate(generation_calls):\n",
    "            print(\n",
    "                f\"   Generation step {i + 1}: seq_len={call['sequence_length']}, \"\n",
    "                f\"new_tokens={call['new_tokens']}\"\n",
    "            )\n",
    "    else:\n",
    "        print(hook_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61bc9a7318a88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
